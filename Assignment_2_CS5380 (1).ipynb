{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ae403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import apache_beam as beam\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.models.param import Param\n",
    "\n",
    "# Constants for data fetching DAG\n",
    "FETCH_BASE_URL = 'https://www.ncei.noaa.gov/data/local-climatological-data/access/'\n",
    "FETCH_YEAR = 2002\n",
    "FETCH_NUM_FILES = 2\n",
    "FETCH_ARCHIVE_OUTPUT_DIR = '/tmp/archives'\n",
    "FETCH_DATA_FILE_OUTPUT_DIR = f'/tmp/data/{FETCH_YEAR}/'\n",
    "FETCH_HTML_FILE_SAVE_DIR = '/tmp/html/'\n",
    "\n",
    "# Constants for analytics DAG\n",
    "ANALYTICS_ARCHIVE_PATH = \"/tmp/archives/2002.zip\"\n",
    "ANALYTICS_REQUIRED_FIELDS = \"WindSpeed, BulbTemperature\"\n",
    "\n",
    "# Default arguments for DAGs\n",
    "default_fetch_args = {\n",
    "    'owner': 'admin',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "default_analytics_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# DAG for fetching data\n",
    "dag_fetch = DAG(\n",
    "    dag_id=\"Fetch_NCEI_Data\",\n",
    "    default_args=default_fetch_args,\n",
    "    description='DataFetch Pipeline',\n",
    "    params={\n",
    "        'base_url': FETCH_BASE_URL,\n",
    "        'year': Param(FETCH_YEAR, type=\"integer\", minimum=1901, maximum=2024),\n",
    "        'num_files': FETCH_NUM_FILES,\n",
    "        'archive_output_dir': FETCH_ARCHIVE_OUTPUT_DIR,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Task 1: Download HTML page\n",
    "fetch_page_task = BashOperator(\n",
    "    task_id=\"Download_HTML_data\",\n",
    "    bash_command=\"curl {{params.base_url}}{{params.year}}/ --create-dirs -o \" + FETCH_HTML_FILE_SAVE_DIR + \"{{params.year}}.html\",\n",
    "    params={'base_url': \"{{ dag_run.conf.get('base_url', params.base_url) }}\"},\n",
    "    dag=dag_fetch,\n",
    ")\n",
    "\n",
    "# Task 2: Select random data files\n",
    "def select_random_files(num_files, base_url, year, **kwargs):\n",
    "    file_save_dir = FETCH_HTML_FILE_SAVE_DIR\n",
    "    with open(f\"{file_save_dir}{year}.html\", \"r\") as f:\n",
    "        page_content = f.read()\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    file_links = [link.get('href') for link in soup.find_all('a') if \".csv\" in link.get('href')]\n",
    "    selected_files = random.sample(file_links, min(int(num_files), len(file_links)))\n",
    "    return [f\"{base_url}{year}/{file}\" for file in selected_files]\n",
    "\n",
    "select_files_task = PythonOperator(\n",
    "    task_id='select_files',\n",
    "    python_callable=select_random_files,\n",
    "    op_kwargs={'num_files': \"{{ dag_run.conf.get('num_files', params.num_files) }}\",\n",
    "               'year': \"{{ dag_run.conf.get('year', params.year) }}\",\n",
    "               'base_url': \"{{ dag_run.conf.get('base_url', params.base_url) }}\"},\n",
    "    dag=dag_fetch,\n",
    ")\n",
    "\n",
    "# Task 3: Download files\n",
    "def download_files(selected_files, **kwargs):\n",
    "    csv_output_dir = FETCH_DATA_FILE_OUTPUT_DIR\n",
    "    os.makedirs(csv_output_dir, exist_ok=True)\n",
    "    for file_url in selected_files:\n",
    "        file_name = urllib.parse.unquote(os.path.basename(file_url))\n",
    "        os.system(f\"curl {file_url} -o {os.path.join(csv_output_dir, file_name)}\")\n",
    "\n",
    "fetch_files_task = PythonOperator(\n",
    "    task_id='fetch_files',\n",
    "    python_callable=download_files,\n",
    "    provide_context=True,\n",
    "    op_kwargs={'csv_output_dir': FETCH_DATA_FILE_OUTPUT_DIR},\n",
    "    dag=dag_fetch,\n",
    ")\n",
    "\n",
    "# Task 4: Zip files\n",
    "zip_files_task = PythonOperator(\n",
    "    task_id='zip_files',\n",
    "    python_callable=lambda output_dir, archive_path: shutil.make_archive(archive_path, 'zip', output_dir),\n",
    "    op_kwargs={'output_dir': FETCH_DATA_FILE_OUTPUT_DIR,\n",
    "               'archive_path': FETCH_DATA_FILE_OUTPUT_DIR.rstrip('/')},\n",
    "    dag=dag_fetch,\n",
    ")\n",
    "\n",
    "# Task 5: Move archive\n",
    "move_archive_task = PythonOperator(\n",
    "    task_id='move_archive',\n",
    "    python_callable=lambda archive_path, target_location: shutil.move(f\"{archive_path}.zip\", os.path.join(target_location, f\"{FETCH_YEAR}.zip\")),\n",
    "    op_kwargs={'target_location': FETCH_ARCHIVE_OUTPUT_DIR,\n",
    "               'archive_path': FETCH_DATA_FILE_OUTPUT_DIR.rstrip('/')},\n",
    "    dag=dag_fetch,\n",
    ")\n",
    "\n",
    "# Set dependencies for the fetch DAG\n",
    "fetch_page_task >> select_files_task >> fetch_files_task >> zip_files_task >> move_archive_task\n",
    "\n",
    "# DAG for analytics\n",
    "dag_analytics = DAG(\n",
    "    dag_id='Analytics_Pipeline',\n",
    "    default_args=default_analytics_args,\n",
    "    description='Analytics pipeline',\n",
    "    params={\n",
    "        'archive_path': ANALYTICS_ARCHIVE_PATH,\n",
    "        'required_fields': ANALYTICS_REQUIRED_FIELDS,\n",
    "    },\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "# Task 2.1: Wait for the archive\n",
    "wait_for_archive_task = FileSensor(\n",
    "    task_id='wait_for_archive',\n",
    "    mode=\"poke\",\n",
    "    poke_interval=5,\n",
    "    timeout=300,  # wait for 5 minutes\n",
    "    filepath=ANALYTICS_ARCHIVE_PATH,\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Task 2.2: Unzip the archive\n",
    "unzip_archive_task = BashOperator(\n",
    "    task_id='unzip_archive',\n",
    "    bash_command=\"unzip -o {{params.archive_path}} -d /tmp/data2\",\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Task 2.3: Process CSV with Apache Beam\n",
    "def process_csv(required_fields, **kwargs):\n",
    "    # Placeholder function body\n",
    "    pass  # Implement CSV processing logic with Apache Beam here\n",
    "\n",
    "process_csv_task = PythonOperator(\n",
    "    task_id='process_csv_files',\n",
    "    python_callable=process_csv,\n",
    "    op_kwargs={'required_fields': \"{{ params.required_fields }}\"},\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Task 2.4: Compute monthly averages\n",
    "def compute_monthly_avg(required_fields, **kwargs):\n",
    "    # Placeholder function body\n",
    "    pass  # Implement monthly averages computation logic here\n",
    "\n",
    "compute_monthly_avg_task = PythonOperator(\n",
    "    task_id='compute_monthly_averages',\n",
    "    python_callable=compute_monthly_avg,\n",
    "    op_kwargs={'required_fields': \"{{ params.required_fields }}\"},\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Task 2.5: Create heatmap visualizations\n",
    "def create_heatmap_visualization(required_fields, **kwargs):\n",
    "    # Placeholder function body\n",
    "    pass  # Implement heatmap visualization logic here\n",
    "\n",
    "create_heatmap_task = PythonOperator(\n",
    "    task_id='create_heatmap_visualization',\n",
    "    python_callable=create_heatmap_visualization,\n",
    "    op_kwargs={'required_fields': \"{{ params.required_fields }}\"},\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Task 2.6: Delete CSV files after processing\n",
    "delete_csv_task = PythonOperator(\n",
    "    task_id='delete_csv_file',\n",
    "    python_callable=lambda: shutil.rmtree('/tmp/data2'),\n",
    "    dag=dag_analytics,\n",
    ")\n",
    "\n",
    "# Set dependencies for the analytics DAG\n",
    "wait_for_archive_task >> unzip_archive_task >> [process_csv_task, compute_monthly_avg_task]\n",
    "process_csv_task >> delete_csv_task\n",
    "compute_monthly_avg_task >> create_heatmap_task >> delete_csv_task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
